# 任务二：使用朴素贝叶斯对搜狗新闻语料库进行分类

## 一、数据库讲解

数据集存放在 `Database` 中，新闻一共分为 9 个类：财经、IT、健康、体育、旅游、教育、招聘、文化、军事，不同种类的新闻分别放在相应代号的文件夹中。NBC.py 中，对新闻进行预处理的模块已经给出，请尝试补全分类器模块的代码（NBC.py 中函数 TextClassifier），自行划分训练集和测试集，对这些新闻进行训练和分类。

## 二、提示

### 1. 数据预处理

对语料库中的文本进行分词，并通过词频找到特征词，从而生成相应的训练集和测试集。这部分代码我们已经给出，请尝试看懂并理解它们。

### 2. 机器学习库

我们鼓励你手写朴素贝叶斯分类器，但是在本任务中，我们允许你使用机器学习库 sklearn 快速实现分类器，以方便研究不同的参数设置对实验结果的影响，探讨影响朴素贝叶斯分类器分类效果的原因。

### 3. 分词工具

本任务需要你安装分词库 jieba。安装方法：pip install jieba。

## 三、提交要求

请尝试在不同的参数下多做几组实验：例如可以在构建词典中尝试删去不同个数的高频词，观察实验结果的变化；也可以在更改不同的特征词数量，观察对分类准确率的影响；或者更改训练集和测试集划分比例，观察不同训练集规模对实验结果的影响；也可以更换特征词的提取方式，如 TF-IDF 等等。编写实验报告，报告中需要包含：实验设置，编程思路，实验结果展示以及自己的思考等。

